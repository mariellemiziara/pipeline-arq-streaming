# -*- coding: utf-8 -*-
"""Arquitetura, pipeline e streaming de dados

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zbdy3wjmYlE-VXY8LsmDqWDqlaPdiyyo
"""

!sudo apt-get update
!sudo apt-get install -y software-properties-common
!sudo apt-add-repository --yes --update ppa:ansible/ansible
!sudo apt-get install -y ansible
!curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
!sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
!sudo apt-get update && sudo apt-get install terraform

from google.colab import auth
auth.authenticate_user()
!gcloud auth application-default login

# Commented out IPython magic to ensure Python compatibility.
# %%writefile main.tf
# provider "google" {
#   project = "502825778152"
#   region  = "us-central1"
# }
# 
# resource "google_storage_bucket" "petadota_images" {
#   name     = "petadota-images-bucket"
#   location = "US"
# }

!ssh-keygen -t rsa -b 4096 -C "seriesmarielle@gmail.com"

cat ~/.ssh/id_rsa.pub

# Commented out IPython magic to ensure Python compatibility.
# %%writefile -a main.tf
# resource "google_compute_instance" "db_vm" {
#   name         = "petadota-db-vm"
#   machine_type = "e2-medium"
#   zone         = "us-central1-a"
# 
#   boot_disk {
#     initialize_params {
#       image = "debian-cloud/debian-11"
#     }
#   }
# 
#   network_interface {
#     network = "default"
#     access_config {
#     }
#   }
# 
#   metadata = {
#     ssh-keys = "ubuntu:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCasyz67uQiHWx7pxinupp1nhndS1kZ0EoLj0Ls7jOmmIwbPNcl9K+0PGqDnEv4dXD7uQ0JslHQPbgYKSgrv/y/gDc4JziWy9WrgtBCtzGP85bnLmnlLbecq8muUQ3ufhQvHON8TzZ4ygbbcrVBCiZ7Nzbx2ZyuypOA0JP6tE72XQR9lcB7SNlO10nji/J3Yt8YZ3RkFOkgamA31bXfAx0GWFdMrSocQIjLtOgRldRABooFCpdRmVwIQW1rI51ReCeCK+NWbasQf9fBP5tLvEz7ejSOWJksybQ3YfdKwbMNLk9iF97DEanX9cG0gw42pQPRheFfiuvVjLKy/5b18rVtKUjnuN9KuyUwSMk4IhLLSjeAL9Rm/NA7Jsvw523Lyu4fUXms5Dyr8kvJ/8KeExGUibBKbnMk0Z4bOzOc6ZUIZkI2Ko75aEqEPXP8mUBk+hrxEwvCF7Om033nTTzKgtq8tfEwpDsnje+HVxY6OuPwnSG6nYmmAhIgAyTkd1OlG4oefxrCsHt+QMWWOo0WOXNvz9TL1jks6jPZhfR9aclNGC0P4tr8QlxPBntT+KwHul3uamMpXWxhtmRZEMYQiroGoBW+C+sH5nuDbdDp3GD9/W96n4Cl1MJ18JF8sBwoeQLhKCPaGb0EtfnsP3/N43/7AhOcoeUjUuQNh4F+RnsRpw== seriesmarielle@gmail.com"
#   }
# }
# 
# output "vm_ip" {
#   value = google_compute_instance.db_vm.network_interface[0].access_config[0].nat_ip
# }

!terraform init
!terraform apply -auto-approve

# Commented out IPython magic to ensure Python compatibility.
# %%writefile setup_mysql.yml
# - hosts: all
#   become: yes
#   tasks:
#     - name: Install MySQL
#       apt:
#         name: mysql-server
#         state: present
# 
#     - name: Start MySQL service
#       service:
#         name: mysql
#         state: started
#         enabled: yes
# 
#     - name: Create Pet_Adota database
#       mysql_db:
#         name: Pet_Adota
#         state: present
# 
#     - name: Create animais_disponiveis table
#       mysql_query:
#         query: |
#           CREATE TABLE animais_disponiveis (
#             id INT AUTO_INCREMENT PRIMARY KEY,
#             raca VARCHAR(255) NOT NULL,
#             url VARCHAR(255) NOT NULL,
#             tipo ENUM('cachorro', 'gato') NOT NULL
#           );
# 
#     - name: Create read-only user
#       mysql_user:
#         name: 'read_user'
#         password: 'readonlypass'
#         priv: 'Pet_Adota.*:SELECT'
#         state: present

!terraform init
!terraform apply -auto-approve

!ssh-keyscan -H 34.31.170.11 >> ~/.ssh/known_hosts

!ansible-playbook -i '34.31.170.11' setup_mysql.yml -u ubuntu --private-key ~/.ssh/id_rsa

"""#Atividade 2 - Pipeline de Imagens da API para o Banco"""

import requests

dog_api_url = "https://dog.ceo/api/breeds/image/random"
dog_images = []

for i in range(10):
    response = requests.get(dog_api_url)
    dog_images.append(response.json()['message'])

cat_api_url = "https://api.thecatapi.com/v1/images/search"
cat_images = []

for i in range(10):
    response = requests.get(cat_api_url)
    cat_images.append(response.json()[0]['url'])

dog_images[:5], cat_images[:5]

!gcloud auth application-default set-quota-project projeto-mari-452814

import json
from google.colab import files
uploaded = files.upload()

import os
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "projeto-mari-452814-d461d7545aae.json"

from google.cloud import storage

client = storage.Client(project="projeto-mari-452814-d461d7545aae")
bucket = client.get_bucket('petadota-images-bucket')

from google.cloud import storage
import os
import requests

client = storage.Client()
bucket = client.get_bucket('petadota-images-bucket')

def download_and_store_image(image_url, animal_type, breed, counter):
    filename = f"{breed}_{str(counter).zfill(2)}.jpg"
    file_path = f"{animal_type}/{breed}/{filename}"

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
    response = requests.get(image_url, headers=headers)
    if response.status_code == 200:
        with open(filename, 'wb') as out_file:
            out_file.write(response.content)
    else:
        raise Exception(f"Failed to download image. Status code: {response.status_code}")

    blob = bucket.blob(file_path)
    blob.upload_from_filename(filename)

    os.remove(filename)

    return file_path

dog_image_paths = []
for idx, dog_url in enumerate(dog_images):
    breed = dog_url.split('/')[4]
    path = download_and_store_image(dog_url, 'cachorro', breed, idx+1)
    dog_image_paths.append(path)

cat_image_paths = []
for idx, cat_url in enumerate(cat_images):
    breed = cat_url.split('/')[4]
    path = download_and_store_image(cat_url, 'gato', breed, idx+1)
    cat_image_paths.append(path)

print(dog_image_paths, cat_image_paths)

from google.cloud import storage

def upload_image_to_gcp(image_path, breed, image_name):
    storage_client = storage.Client()

    bucket_name = "petadota-images-bucket"
    bucket = storage_client.get_bucket(bucket_name)

    destination_blob_name = f"pet-adota-imagens/{'cachorro' if breed != 'gato' else 'gato'}/{breed}/{image_name}"

    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(image_path)
    print(f"Image {image_name} uploaded to GCP at {destination_blob_name}")

!pip install pymysql

def register_image_in_db(breed, image_name, image_url, tipo):
    connection = pymysql.connect(
        host='34.31.170.11',
        user='root',
        password='',
        database='Pet_Adota'
    )

    cursor = connection.cursor()
    query = "INSERT INTO animais_disponiveis (raca, url, tipo) VALUES (%s, %s, %s)"
    cursor.execute(query, (breed, image_url, tipo))
    connection.commit()
    cursor.close()
    connection.close()
    print(f"Metadata for {image_name} has been registered in the database.")

!pip install apache-airflow

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(seconds=5),
}

dag = DAG(
    'pipeline_imagens_pet_adota',
    default_args=default_args,
    description='Pipeline para baixar imagens de cães e gatos, salvar no GCP e registrar no MySQL',
    schedule_interval=timedelta(seconds=30),
    start_date=datetime(2025, 2, 28),
    catchup=False,
)

def pipeline():
    download_dog_images()
    download_cat_images()

pipeline_task = PythonOperator(
    task_id='run_pipeline',
    python_callable=pipeline,
    dag=dag,
)

pipeline_task

!airflow users create \
    --username admin \
    --firstname Marielle \
    --lastname Miziara \
    --role Admin \
    --email seriesmarielle@gmail.com

!airflow db init
!airflow webserver -p 8081
!airflow scheduler

"""#Atividade 3 - Processamento de Interações de Usuários em Tempo Real"""

!wget https://archive.apache.org/dist/kafka/3.6.1/kafka_2.13-3.6.1.tgz
!tar -xzf kafka_2.13-3.6.1.tgz
!mv kafka_2.13-3.6.1 kafka

!kafka/bin/zookeeper-server-start.sh -daemon kafka/config/zookeeper.properties
!kafka/bin/kafka-server-start.sh -daemon kafka/config/server.properties

!kafka/bin/kafka-topics.sh --create --topic interacoes_animais --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1

!pip install kafka-python

from kafka import KafkaProducer
import json
import time
import random

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

racas_cachorro = ["labrador", "SRD", "poodle", "beagle", "husky"]
racas_gato = ["siames", "persa", "sphynx", "ragdoll", "bengal"]

def gerar_evento():
    usuario_id = random.randint(1, 1000)
    tipo = random.choice(["cachorro", "gato"])
    raca = random.choice(racas_cachorro if tipo == "cachorro" else racas_gato)
    timestamp = int(time.time())

    evento = {
        "usuario_id": usuario_id,
        "raca": raca,
        "tipo": tipo,
        "timestamp": timestamp
    }

    return evento

for _ in range(100):
    evento = gerar_evento()
    producer.send('interacoes_animais', evento)
    print(f"Evento publicado: {evento}")
    time.sleep(1)

!.option("kafka.bootstrap.servers", "VM_EXTERNAL_IP:9092")

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, window
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

spark = SparkSession.builder.appName("MonitoramentoAdocoes").getOrCreate()

data = [
    (1, "Labrador", "cachorro", 1700000000),
    (2, "Persa", "gato", 1700000020),
    (3, "Labrador", "cachorro", 1700000050),
]

schema = StructType([
    StructField("usuario_id", IntegerType()),
    StructField("raca", StringType()),
    StructField("tipo", StringType()),
    StructField("timestamp", IntegerType())
])

df_json = spark.createDataFrame(data, schema)

tendencias = df_json.groupBy("raca", "tipo").agg(count("*").alias("visualizacoes"))

tendencias.show()

from pyspark.sql.functions import col, expr
df_filtrado = df_json.filter(col("timestamp") >= expr("current_timestamp() - INTERVAL 1 MINUTE"))

df_filtrado.show()

spark = SparkSession.builder \
    .appName("SalvarEstatisticasMySQL") \
    .config("spark.jars.packages", "mysql:mysql-connector-java:8.0.33") \
    .getOrCreate()

!telnet 34.31.170.11 3306

!curl ifconfig.me

!sudo ufw allow 3306/tcp
!sudo ufw reload

!pip install pymysql sqlalchemy
!pip install cloud-sql-python-connector

from google.cloud.sql.connector import Connector
import sqlalchemy

instance_connection_name = "projeto-mari-452814:us-central1:colab"
db_user = "root"
db_pass = ""

connector = Connector()

def getconn():
    conn = connector.connect(
        instance_connection_name,
        "pymysql",
        user=db_user,
        password=db_pass
    )
    return conn

pool = sqlalchemy.create_engine(
    "mysql+pymysql://",
    creator=getconn,
)

with pool.connect() as db_conn:
    db_conn.execute("CREATE DATABASE IF NOT EXISTS Pet_Adota")
    print("Banco de dados 'Pet_Adota' criado com sucesso!")

import pandas as pd

data = [
    ("labrador", "cachorro", 100),
    ("siames", "gato", 50),
    ("poodle", "cachorro", 75),
]

df_estatisticas = pd.DataFrame(data, columns=["raca", "tipo", "visualizacoes"])
print(df_estatisticas)

from google.cloud.sql.connector import Connector
import pymysql
import sqlalchemy
import pandas as pd
from sqlalchemy import create_engine
from sqlalchemy.engine import base

instance_connection_name = "projeto-mari-452814:us-central1:colab"
db_user = "root"
db_pass = ""
db_name = "Pet_Adota"

connector = Connector()

def getconn():
    conn = connector.connect(
        instance_connection_name,
        "pymysql",
        user=db_user,
        password=db_pass,
        db=db_name
    )
    return conn

class MySQLConnection(base.Connection):
    def __init__(self, conn):
        self.conn = conn
        self.cursor = conn.cursor()

    def execute(self, sql, params=None):
        return self.cursor.execute(sql, params)

    def close(self):
        self.cursor.close()
        self.conn.close()


def get_sqlalchemy_engine():
    return create_engine(
        "mysql+pymysql://",
        creator=lambda: MySQLConnection(getconn())
    )

pool = get_sqlalchemy_engine()

data = [
    ("labrador", "cachorro", 100),
    ("siames", "gato", 50),
    ("poodle", "cachorro", 75),
]

df_estatisticas = pd.DataFrame(data, columns=["raca", "tipo", "visualizacoes"])

df_estatisticas.to_sql(
    name="estatisticas_animais",
    con=pool,
    if_exists="append",
    index=False
)

print("Estatísticas salvas no MySQL com sucesso!")

import pymysql
import pandas as pd

host = "35.188.171.159"
user = "root"
password = ""
database = "Pet_Adota"

data = [
    ("labrador", "cachorro", 100),
    ("siames", "gato", 50),
    ("poodle", "cachorro", 75),
]

df_estatisticas = pd.DataFrame(data, columns=["raca", "tipo", "visualizacoes"])

connection = None

try:
    connection = pymysql.connect(
        host=host,
        user=user,
        password=password,
        database=database
    )
    print("Conexão bem-sucedida!")

    cursor = connection.cursor()

    create_table_query = """
    CREATE TABLE IF NOT EXISTS estatisticas_animais (
        id INT AUTO_INCREMENT PRIMARY KEY,
        raca VARCHAR(255) NOT NULL,
        tipo VARCHAR(255) NOT NULL,
        visualizacoes INT NOT NULL,
        data_registro TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    """
    cursor.execute(create_table_query)
    print("Tabela 'estatisticas_animais' criada com sucesso!")

    for index, row in df_estatisticas.iterrows():
        insert_query = """
        INSERT INTO estatisticas_animais (raca, tipo, visualizacoes)
        VALUES (%s, %s, %s)
        """
        cursor.execute(insert_query, (row["raca"], row["tipo"], row["visualizacoes"]))

    connection.commit()
    print("Estatísticas salvas no MySQL com sucesso!")

except Exception as e:
    print(f"Erro ao conectar ou salvar dados no MySQL: {e}")

finally:
    if connection:
        connection.close()
        print("Conexão fechada.")

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import pymysql

def processar_tendencias():
    host = "35.188.171.159"
    user = "root"
    password = ""
    database = "Pet_Adota"

    try:
        connection = pymysql.connect(
            host=host,
            user=user,
            password=password,
            database=database
        )
        print("Conexão bem-sucedida!")

        query = """
        SELECT raca, tipo, SUM(visualizacoes) AS total_visualizacoes
        FROM estatisticas_animais
        GROUP BY raca, tipo
        ORDER BY total_visualizacoes DESC;
        """
        df_tendencias = pd.read_sql(query, connection)
        print("Tendências encontradas:")
        print(df_tendencias)

    except Exception as e:
        print(f"Erro ao processar tendências: {e}")

    finally:
        if connection:
            connection.close()
            print("Conexão fechada.")

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 10, 25),
    'retries': 1,
    'retry_delay': timedelta(seconds=5),
}

dag = DAG(
    'visualizar_tendencias',
    default_args=default_args,
    description='DAG para visualizar tendências a cada 30 segundos',
    schedule=timedelta(seconds=30),  # Usando 'schedule' em vez de 'schedule_interval'
    catchup=False,
)

tarefa_processar_tendencias = PythonOperator(
    task_id='processar_tendencias',
    python_callable=processar_tendencias,
    dag=dag,
)

tarefa_processar_tendencias